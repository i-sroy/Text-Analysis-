{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the necessary libraries\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "import lxml\n",
    "import requests\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignoring the ssl certificate errors\n",
    "import ssl\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping multiple websites from the list of sites given and storing the scraped data in different text files\n",
    "\n",
    "def scrape(u):      #function to scrape the data using beautifulsoup\n",
    "    url = i\n",
    "    print(url)\n",
    "    headers = {'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36'}\n",
    "    response = requests.get(url,headers=headers)\n",
    "    soup = BeautifulSoup(response.text,'html.parser')\n",
    "    return soup\n",
    "\n",
    "def make_file(data,fname):      #function to store the scraped data in different scraped files    \n",
    "    filename = str(fname)+\".txt\"\n",
    "    f = open(filename,'a+')\n",
    "    title = data.find('title').text.strip()     #scraping the title\n",
    "    print(title)\n",
    "    f.write(title+'\\n')\n",
    "        \n",
    "    art_text = data.find_all('p')       #scraping the body\n",
    "    for j in art_text:\n",
    "       #print(i.text.strip())\n",
    "        f.write(j.text.strip())\n",
    "    f.close()\n",
    "\n",
    "df = pd.read_excel('Input.xlsx')\n",
    "\n",
    "for i in df['URL']:\n",
    "    scraped_data = scrape(i)\n",
    "    df2 = df.loc[df['URL']== i,'URL_ID']\n",
    "    p = list(df2.values)\n",
    "    id = p.pop(0)\n",
    "    make_file(scraped_data,id)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the postive and negative words text files and storing them in two different lists\n",
    "f_pos = open('positive-words.txt','r')\n",
    "content = f_pos.readlines()\n",
    "positive_words = []\n",
    "for i in content:\n",
    "    positive_words.append(i.rstrip())      \n",
    "#print(positive_words)\n",
    "f_pos.close()\n",
    "\n",
    "f_neg = open('negative-words.txt','r',encoding=\"ISO-8859-1\")\n",
    "content1 = f_neg.readlines()\n",
    "negative_words = []\n",
    "for i in content1:\n",
    "    negative_words.append(i.rstrip())\n",
    "f_neg.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stopwords using nltk package as mentioned in the obejctives.docx and calculating the variables.\n",
    "def remove(i):\n",
    "    filename = str(i)+\".txt\"\n",
    "    f = open(filename,'r')\n",
    "    content = f.readlines()\n",
    "    url_file_content = \"\"\n",
    "    for j in content:\n",
    "        url_file_content = url_file_content+\" \"+j\n",
    "        \n",
    "    stop_words = set(stopwords.words('english'))    \n",
    "\n",
    "    word_tokens = word_tokenize(url_file_content)\n",
    "\n",
    "    modified_content = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:         \n",
    "            modified_content.append(w)\n",
    "    return modified_content,url_file_content\n",
    "\n",
    "#calculating the variables for all the files and storing them in separate lists.\n",
    "\n",
    "word_count = []\n",
    "positive_score = []\n",
    "negative_score = []\n",
    "polarity_Score = []\n",
    "subjectivity_score = []\n",
    "complex_word_count = []\n",
    "avg_sentence_length = []\n",
    "perc_of_complex_words = []\n",
    "fog_index = []\n",
    "avg_words_per_sent = []\n",
    "personal_pronoun_count = []\n",
    "avg_word_length = []\n",
    "\n",
    "for i in df['URL_ID']:\n",
    "    modified_content,url_file_content = remove(i) \n",
    "    wc = len(modified_content)\n",
    "    word_count.append(wc)\n",
    "\n",
    "    p_score = 0\n",
    "    n_score = 0\n",
    "\n",
    "#print(url_file_content)\n",
    "    sent = sent_tokenize(url_file_content)\n",
    "    sentence_count = len(sent)   #calculating the sentence length\n",
    "    complex_word = []\n",
    "    personal_pronoun = []\n",
    "    char = 0\n",
    "    char_count = 0\n",
    "\n",
    "    for word in url_file_content.split():\n",
    "        if word in positive_words:\n",
    "            p_score = p_score + 1           #calculating positive score for each file\n",
    "        if word in negative_words:\n",
    "            n_score = n_score - 1           #calculating negative score for each file\n",
    "\n",
    "        #calculating number of syllable for each word in a file\n",
    "        syllable_count=0\n",
    "\n",
    "        for w in word:\n",
    "            #print(j)\n",
    "            if(w=='a' or w=='e' or w=='i' or w=='o' or w=='u' or w=='A' or w=='E' or w=='I' or w=='O' or w=='U'):\n",
    "                syllable_count=syllable_count+1         \n",
    "                #print(syllable_count)\n",
    "        if syllable_count>2:        \n",
    "            complex_word.append(word)       #creating a list of complex words for each file\n",
    "        \n",
    "        #calculating the count of personal pronouns in each file\n",
    "        if (word=='I' or word=='i' or word=='we' or word=='We'or word=='my' or word=='My' or word=='ours' or word=='Ours' or word=='us' or word=='Us'):\n",
    "            personal_pronoun.append(word)       \n",
    "        for c in word:\n",
    "            char = char+1\n",
    "        char_count = char_count+char\n",
    "\n",
    "    #appending all the calculated variables for each file and appending them in separate lists.\n",
    "    #These lists, at the end will contain the value of variables of all the files. \n",
    "    avg_wlen = char_count/wc\n",
    "    avg_word_length.append(avg_wlen)\n",
    "\n",
    "    personal_pcount = len(personal_pronoun)\n",
    "    personal_pronoun_count.append(personal_pcount)\n",
    "\n",
    "    com_len = len(complex_word)\n",
    "    complex_word_count.append(com_len)\n",
    "\n",
    "    avg_sent_len = wc/sentence_count\n",
    "    avg_sentence_length.append(avg_sent_len)\n",
    "\n",
    "    perc_of_complex = com_len/wc\n",
    "    perc_of_complex_words.append(perc_of_complex)\n",
    "\n",
    "    f_index = 0.4 * (avg_sent_len + perc_of_complex)\n",
    "    fog_index.append(f_index)\n",
    "\n",
    "    avg_words = wc/sentence_count\n",
    "    avg_words_per_sent.append(avg_words)\n",
    "\n",
    "    positive_score.append(p_score)\n",
    "    negative_score.append(n_score)\n",
    "\n",
    "    polar_Score = ((p_score - n_score)/((p_score + n_score) + 0.000001))\n",
    "    polarity_Score.append(polar_Score)\n",
    "\n",
    "    sub_score = (p_score + n_score)/ ((wc) + 0.000001)\n",
    "    subjectivity_score.append(sub_score)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing the calculated variables in output data structure.xlxs as mentioned in the instructions.\n",
    "old_df = pd.read_excel('Output Data Structure.xlsx')  \n",
    "new_df = pd.DataFrame({'POSITIVE SCORE':positive_score,'NEGATIVE SCORE':negative_score,'POLARITY SCORE':polarity_Score,'SUBJECTIVITY SCORE':subjectivity_score,'AVG SENTENCE LENGTH':avg_sentence_length,'PERCENTAGE OF COMPLEX WORDS':perc_of_complex_words,'FOG INDEX':fog_index,\n",
    "            'AVERAGE NUMBER OF WORDS':avg_words_per_sent,'COMPLEX WORD COUNT' : complex_word_count, 'WORD_COUNT' : word_count,'PERSONAL PRONOUN COUNT':personal_pronoun_count,'AVERAGE WORD LENGTH':avg_word_length})\n",
    "frames = [old_df,new_df]\n",
    "result = pd.concat(frames, axis=1)\n",
    "result.to_excel('Output Data Structure.xlsx',sheet_name=\"sheet_1\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
